<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Overfitting in ML</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="../../stylesheet.css">
    <link rel="stylesheet" type="text/css" href="https://static.javatpoint.com/link.css?v=6.0" async />
    <link rel="icon" type="image/x-icon" href="../icon/sg.png">
    <style>
        .active-submenu{
            font-weight: 500;
            background-color: darkgray;
        }
        .active-submenu:hover{
            font-weight: 500;
            background-color: darkgray;
        }
        a:hover{
            text-decoration: none;
        }
    </style>
</head>

<body style="background-image: radial-gradient(#b3d6e6, white, #b3d6e6);">
    <nav class="navbar navbar-dark bg-primary sticky-top" aria-label="Dark offcanvas navbar">
        <div class="container-fluid">
            <a class="navbar-brand" href="../../home.html">
                <img src="../icon/sg.png" alt="icon" style="height: 50px; width: 50px; margin-left: 20px;">
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="offcanvas"
                data-bs-target="#offcanvasNavbarDark" aria-controls="offcanvasNavbarDark"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="offcanvas offcanvas-end text-bg-dark" tabindex="-1" id="offcanvasNavbarDark"
                aria-labelledby="offcanvasNavbarDarkLabel">
                <div class="offcanvas-header">
                    <h5 class="offcanvas-title" id="offcanvasNavbarDarkLabel">
                        <img src="../icon/sg.png" alt="icon" style="height: 50px;">
                    </h5>
                    <button type="button" class="btn-close btn-close-white" data-bs-dismiss="offcanvas"
                        aria-label="Close"></button>
                </div>
                <div class="offcanvas-body">
                    <ul class="navbar-nav justify-content-end flex-grow-1 pe-3">
                        <li class="nav-item">
                            <a class="nav-link" aria-current="page" href="../../home.html">Home</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Programming Language
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/c_page.html">C</a></li>
                                <li><a class="dropdown-item" href="../../first_page/c++_page.html">C++</a></li>
                                <li><a class="dropdown-item" href="../../first_page/java_page.html">Java</a></li>
                                <li><a class="dropdown-item" href="../../first_page/python_page.html">Python</a></li>
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Web Developement
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/html_page.html">HTML</a></li>
                                <li><a class="dropdown-item" href="../../first_page/css_page.html">CSS</a></li>
                                <li><a class="dropdown-item" href="../../first_page/javascript_page.html">JavaScript</a>
                                </li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../first_page/ml_page.html">Machine Learning</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link active dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Machine Learning
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="1_what_is_machine_learning.html">What is Machine Learning</a></li>
                                <li><a class="dropdown-item" href="2_application_of_ml.html">Applications of ML</a></li>
                                <li><a class="dropdown-item" href="3_life_cycle.html">Machine Learning Life Cycle</a></li>
                                <li><a class="dropdown-item" href="4_installing_anaconda_python.html">Installing Anaconda & Python</a></li>
                                <li><a class="dropdown-item" href="5_ai_vs_ml.html">Artificial Intelligence vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="6_datasets.html">Datasets</a></li>
                                <li><a class="dropdown-item" href="7_data_preprocessing.html">Data preprocessing</a></li>
                                <li><a class="dropdown-item" href="8_supervised_learning.html">Supervised Learning</a></li>
                                <li><a class="dropdown-item" href="9_unsupervised_learning.html">Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="10_supervised_vs_unsupervised.html">Supervised Learning vs Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="11_regression_analysis.html">Regression Analysis</a></li>
                                <li><a class="dropdown-item" href="12_linear_regression.html">Linear Regression</a></li>
                                <li><a class="dropdown-item" href="13_simple_linear_regression.html">Simple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="14_multiple_linear_regression.html">Multiple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="15_polynomial_regression.html">Polynomial Regression</a></li>
                                <li><a class="dropdown-item" href="16_classification_algorithm.html">Classification</a></li>
                                <li><a class="dropdown-item" href="17_logistic_regression.html">Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="18_knn.html">KNN Algorithm</a></li>
                                <li><a class="dropdown-item" href="19_svm.html">Support Vector Machine</a></li>
                                <li><a class="dropdown-item" href="20_naive_bayes.html">Naive Bayes</a></li>
                                <li><a class="dropdown-item" href="21_regression_vs_classification.html">Regression vs Classification</a></li>
                                <li><a class="dropdown-item" href="22_linear_reg_vs_logistic_reg.html">Linear Regression vs Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="23_decision_tree.html">Decision Tree</a></li>
                                <li><a class="dropdown-item" href="24_random_forest.html">Random Forest</a></li>
                                <li><a class="dropdown-item" href="25_clustering.html">Clustering</a></li>
                                <li><a class="dropdown-item" href="26_k_means_clustering.html">K-Means Clustering</a></li>
                                <li><a class="dropdown-item" href="27_confusion_matrix.html">Confusion Matrix</a></li>
                                <li><a class="dropdown-item" href="28_cross_validation.html">Cross Validation</a></li>
                                <li><a class="dropdown-item" href="29_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                                <li><a class="dropdown-item" href="30_overfitting_underfitting.html">Overfitting & Underfitting</a></li>
                                <li><a class="dropdown-item" href="31_principle_component_analysis.html">Principle Component Analysis</a></li>
                                <li><a class="dropdown-item" href="32_p_value.html">P-Value</a></li>
                                <li><a class="dropdown-item" href="33_regularization.html">Regularization in ML</a></li>
                                <li><a class="dropdown-item active-submenu" href="34_overfitting.html">Overfitting in ML</a></li>
                                <li><a class="dropdown-item" href="35_bias_variance.html">Bias & Variance</a></li>
                                <li><a class="dropdown-item" href="36_gradient_descent.html">Gradient Descent</a></li>
                                <li><a class="dropdown-item" href="37_cost_function.html">Cost Function</a></li>
                                <li><a class="dropdown-item" href="38_normalization.html">Normalization in ML</a></li>
                                <li><a class="dropdown-item" href="39_epoch_batch_iterations.html">Epoch, Batch & Iterations</a></li>
                                <li><a class="dropdown-item" href="40_feature_engineering.html">Feature Engineering</a></li>
                                <li><a class="dropdown-item" href="41_perceptron.html">Perceptron</a></li>
                                <li><a class="dropdown-item" href="42_data_science_vs_ml.html">Data Science vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="43_ml_vs_deep_learning.html">Machine Learning vs Deep Learning</a></li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../about.html">About Us</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../contact.html">Contact Us</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </nav>
    <div style="padding: 30px;font-size: 18px;">
    <h1 class="h1">Overfitting in Machine Learning</h1>
    <p>In the real world, the dataset present will never be clean and perfect. It means each dataset contains
        impurities, noisy data, outliers, missing data, or imbalanced data. Due to these impurities, different problems
        occur that affect the accuracy and the performance of the model. One of such problems is Overfitting in Machine
        Learning. <em>Overfitting is a problem that a model can exhibit.</em></p>
    <blockquote>A statistical model is said to be overfitted if it can&rsquo;t generalize well with unseen data.
    </blockquote>
    <p>Before understanding overfitting, we need to know some basic terms, which are:</p>
    <p><strong>Noise</strong>: Noise is meaningless or irrelevant data present in the dataset. It affects the
        performance of the model if it is not removed.</p>
    <p><strong>Bias</strong>: Bias is a prediction error that is introduced in the model due to oversimplifying the
        machine learning algorithms. Or it is the difference between the predicted values and the actual values.</p>
    <p><strong>Variance</strong>: If the machine learning model performs well with the training dataset, but does not
        perform well with the test dataset, then variance occurs.</p>
    <p><strong>Generalization</strong>: It shows how well a model is trained to predict unseen data.</p>
    <h2 class="h2">What is Overfitting?</h2>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-in-machine-learning.png"
        alt="Overfitting in Machine Learning" />
    <ul class="points">
        <li>Overfitting &amp; underfitting are the two main errors/problems in the machine learning model, which cause
            poor performance in Machine Learning.</li>
        <li>Overfitting occurs when the model fits more data than required, and it tries to capture each and every
            datapoint fed to it. Hence it starts capturing noise and inaccurate data from the dataset, which degrades
            the performance of the model.</li>
        <li>An overfitted model doesn't perform accurately with the test/unseen dataset and can&rsquo;t generalize well.
        </li>
        <li>An overfitted model is said to have low bias and high variance.</li>
    </ul>
    <h2 class="h2">Example to Understand Overfitting</h2>
    <p>We can understand overfitting with a general example. Suppose there are three students, X, Y, and Z, and all
        three are preparing for an exam. X has studied only three sections of the book and left all other sections. Y
        has a good memory, hence memorized the whole book. And the third student, Z, has studied and practiced all the
        questions. So, in the exam, X will only be able to solve the questions if the exam has questions related to
        section 3. Student Y will only be able to solve questions if they appear exactly the same as given in the book.
        Student Z will be able to solve all the exam questions in a proper way. </p>
    <p>The same happens with machine learning; if the algorithm learns from a small part of the data, it is unable to
        capture the required data points and hence under fitted. </p>
    <p>Suppose the model learns the training dataset, like the Y student. They perform very well on the seen dataset but
        perform badly on unseen data or unknown instances. In such cases, the model is said to be Overfitting. </p>
    <p>And if the model performs well with the training dataset and also with the test/unseen dataset, similar to
        student Z, it is said to be a good fit. </p>
    <h2 class="h2">How to detect Overfitting?</h2>
    <p>Overfitting in the model can only be detected once you test the data. To detect the issue, we can perform
        <strong>Train/test split. </strong></p>
    <p>In the train-test split of the dataset, we can divide our dataset into random test and training datasets. We
        train the model with a training dataset which is about 80% of the total dataset. After training the model, we
        test it with the test dataset, which is 20 % of the total dataset.</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-in-machine-learning2.png"
        alt="Overfitting in Machine Learning" />
    <p>Now, if the model performs well with the training dataset but not with the test dataset, then it is likely to
        have an overfitting issue.</p>
    <p>For example, if the model shows 85% accuracy with training data and 50% accuracy with the test dataset, it means
        the model is not performing well.</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-in-machine-learning3.jpg"
        alt="Overfitting in Machine Learning" />
    <h2 class="h2">Ways to prevent the Overfitting</h2>
    <p>Although overfitting is an error in Machine learning which reduces the performance of the model, however, we can
        prevent it in several ways. With the use of the linear model, we can avoid overfitting; however, many real-world
        problems are non-linear ones. It is important to prevent overfitting from the models. Below are several ways
        that can be used to prevent overfitting:</p>
    <ol class="points">
        <li><strong>Early Stopping</strong></li>
        <li><strong>Train with more data</strong></li>
        <li><strong>Feature Selection</strong></li>
        <li><strong>Cross-Validation</strong></li>
        <li><strong>Data Augmentation</strong></li>
        <li><strong>Regularization</strong></li>
    </ol>
    <h3 class="h3">Early Stopping</h3>
    <p>In this technique, the training is paused before the model starts learning the noise within the model. In this
        process, while training the model iteratively, measure the performance of the model after each iteration.
        Continue up to a certain number of iterations until a new iteration improves the performance of the model.</p>
    <p>After that point, the model begins to overfit the training data; hence we need to stop the process before the
        learner passes that point.</p>
    <p>Stopping the training process before the model starts capturing noise from the data is known as <strong>early
            stopping.</strong></p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/overfitting-in-machine-learning4.png"
        alt="Overfitting in Machine Learning" />
    <p>However, this technique may lead to the underfitting problem if training is paused too early. So, it is very
        important to find that "sweet spot" between underfitting and overfitting. </p>
    <h3 class="h3">Train with More data</h3>
    <p>Increasing the training set by including more data can enhance the accuracy of the model, as it provides more
        chances to discover the relationship between input and output variables.</p>
    <p>It may not always work to prevent overfitting, but this way helps the algorithm to detect the signal better to
        minimize the errors.</p>
    <p>When a model is fed with more training data, it will be unable to overfit all the samples of data and forced to
        generalize well.</p>
    <p>But in some cases, the additional data may add more noise to the model; hence we need to be sure that data is
        clean and free from in-consistencies before feeding it to the model.</p>
    <h3 class="h3">Feature Selection</h3>
    <p>While building the ML model, we have a number of parameters or features that are used to predict the outcome.
        However, sometimes some of these features are redundant or less important for the prediction, and for this
        feature selection process is applied. In the feature selection process, we identify the most important features
        within training data, and other features are removed. Further, this process helps to simplify the model and
        reduces noise from the data. Some algorithms have the auto-feature selection, and if not, then we can manually
        perform this process.</p>
    <h3 class="h3">Cross-Validation</h3>
    <p>Cross-validation is one of the powerful techniques to prevent overfitting.</p>
    <p>In the general k-fold cross-validation technique, we divided the dataset into k-equal-sized subsets of data;
        these subsets are known as folds.</p>
    <h3 class="h3">Data Augmentation</h3>
    <p>Data Augmentation is a data analysis technique, which is an alternative to adding more data to prevent
        overfitting. In this technique, instead of adding more training data, slightly modified copies of already
        existing data are added to the dataset.</p>
    <p>The data augmentation technique makes it possible to appear data sample slightly different every time it is
        processed by the model. Hence each data set appears unique to the model and prevents overfitting.</p>
    <h3 class="h3">Regularization</h3>
    <p>If overfitting occurs when a model is complex, we can reduce the number of features. However, overfitting may
        also occur with a simpler model, more specifically the Linear model, and for such cases, regularization
        techniques are much helpful.</p>
    <p>Regularization is the most popular technique to prevent overfitting. It is a group of methods that forces the
        learning algorithms to make a model simpler. Applying the regularization technique may slightly increase the
        bias but slightly reduces the variance. In this technique, we modify the objective function by adding the
        penalizing term, which has a higher value with a more complex model.</p>
    <p>The two commonly used regularization techniques are L1 Regularization and L2 Regularization.</p>
    <h3 class="h3">Ensemble Methods</h3>
    <p>In ensemble methods, prediction from different machine learning models is combined to identify the most popular
        result.</p>
    <p>The most commonly used ensemble methods are <strong>Bagging and Boosting.</strong></p>
    <p>In bagging, individual data points can be selected more than once. After the collection of several sample
        datasets, these models are trained independently, and depending on the type of task-i.e., regression or
        classification-the average of those predictions is used to predict a more accurate result. Moreover, bagging
        reduces the chances of overfitting in complex models.</p>
    <p>In boosting, a large number of weak learners arranged in a sequence are trained in such a way that each learner
        in the sequence learns from the mistakes of the learner before it. It combines all the weak learners to come out
        with one strong learner. In addition, it improves the predictive flexibility of simple models.</p>
    <hr />
</div>
<div class="d-flex justify-content-between align-items-center my-4 mx-5">
        <a href="33_regularization.html"><button class="btn btn-danger" type="button">Previous</button></a>
        <a href="35_bias_variance.html"><button class="btn btn-success" type="button">Next</button></a>
    </div>
    <footer>
        <div class="container-fluid bg-dark text-bg-dark"
            style="height: 50px; display: flex; align-items: center; font-size: 15px; justify-content: center;">
            Copyright&nbsp; ©&nbsp; 2023&nbsp; StudyGenie&nbsp; -&nbsp; All Rights Reserved.
        </div>
    </footer>
</body>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
    crossorigin="anonymous"></script>
</html>