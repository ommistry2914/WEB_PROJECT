<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cross Validation</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="../../stylesheet.css">
    <link rel="stylesheet" type="text/css" href="https://static.javatpoint.com/link.css?v=6.0" async />
    <link rel="icon" type="image/x-icon" href="../icon/sg.png">
    <style>
        .active-submenu{
            font-weight: 500;
            background-color: darkgray;
        }
        .active-submenu:hover{
            font-weight: 500;
            background-color: darkgray;
        }
        a:hover{
            text-decoration: none;
        }
    </style>
</head>

<body style="background-image: radial-gradient(#b3d6e6, white, #b3d6e6);">
    <nav class="navbar navbar-dark bg-primary sticky-top" aria-label="Dark offcanvas navbar">
        <div class="container-fluid">
            <a class="navbar-brand" href="../../home.html">
                <img src="../icon/sg.png" alt="icon" style="height: 50px; width: 50px; margin-left: 20px;">
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="offcanvas"
                data-bs-target="#offcanvasNavbarDark" aria-controls="offcanvasNavbarDark"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="offcanvas offcanvas-end text-bg-dark" tabindex="-1" id="offcanvasNavbarDark"
                aria-labelledby="offcanvasNavbarDarkLabel">
                <div class="offcanvas-header">
                    <h5 class="offcanvas-title" id="offcanvasNavbarDarkLabel">
                        <img src="../icon/sg.png" alt="icon" style="height: 50px;">
                    </h5>
                    <button type="button" class="btn-close btn-close-white" data-bs-dismiss="offcanvas"
                        aria-label="Close"></button>
                </div>
                <div class="offcanvas-body">
                    <ul class="navbar-nav justify-content-end flex-grow-1 pe-3">
                        <li class="nav-item">
                            <a class="nav-link" aria-current="page" href="../../home.html">Home</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Programming Language
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/c_page.html">C</a></li>
                                <li><a class="dropdown-item" href="../../first_page/c++_page.html">C++</a></li>
                                <li><a class="dropdown-item" href="../../first_page/java_page.html">Java</a></li>
                                <li><a class="dropdown-item" href="../../first_page/python_page.html">Python</a></li>
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Web Developement
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/html_page.html">HTML</a></li>
                                <li><a class="dropdown-item" href="../../first_page/css_page.html">CSS</a></li>
                                <li><a class="dropdown-item" href="../../first_page/javascript_page.html">JavaScript</a>
                                </li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../first_page/ml_page.html">Machine Learning</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link active dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Machine Learning
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="1_what_is_machine_learning.html">What is Machine Learning</a></li>
                                <li><a class="dropdown-item" href="2_application_of_ml.html">Applications of ML</a></li>
                                <li><a class="dropdown-item" href="3_life_cycle.html">Machine Learning Life Cycle</a></li>
                                <li><a class="dropdown-item" href="4_installing_anaconda_python.html">Installing Anaconda & Python</a></li>
                                <li><a class="dropdown-item" href="5_ai_vs_ml.html">Artificial Intelligence vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="6_datasets.html">Datasets</a></li>
                                <li><a class="dropdown-item" href="7_data_preprocessing.html">Data preprocessing</a></li>
                                <li><a class="dropdown-item" href="8_supervised_learning.html">Supervised Learning</a></li>
                                <li><a class="dropdown-item" href="9_unsupervised_learning.html">Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="10_supervised_vs_unsupervised.html">Supervised Learning vs Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="11_regression_analysis.html">Regression Analysis</a></li>
                                <li><a class="dropdown-item" href="12_linear_regression.html">Linear Regression</a></li>
                                <li><a class="dropdown-item" href="13_simple_linear_regression.html">Simple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="14_multiple_linear_regression.html">Multiple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="15_polynomial_regression.html">Polynomial Regression</a></li>
                                <li><a class="dropdown-item" href="16_classification_algorithm.html">Classification</a></li>
                                <li><a class="dropdown-item" href="17_logistic_regression.html">Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="18_knn.html">KNN Algorithm</a></li>
                                <li><a class="dropdown-item" href="19_svm.html">Support Vector Machine</a></li>
                                <li><a class="dropdown-item" href="20_naive_bayes.html">Naive Bayes</a></li>
                                <li><a class="dropdown-item" href="21_regression_vs_classification.html">Regression vs Classification</a></li>
                                <li><a class="dropdown-item" href="22_linear_reg_vs_logistic_reg.html">Linear Regression vs Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="23_decision_tree.html">Decision Tree</a></li>
                                <li><a class="dropdown-item" href="24_random_forest.html">Random Forest</a></li>
                                <li><a class="dropdown-item" href="25_clustering.html">Clustering</a></li>
                                <li><a class="dropdown-item" href="26_k_means_clustering.html">K-Means Clustering</a></li>
                                <li><a class="dropdown-item" href="27_confusion_matrix.html">Confusion Matrix</a></li>
                                <li><a class="dropdown-item active-submenu" href="28_cross_validation.html">Cross Validation</a></li>
                                <li><a class="dropdown-item" href="29_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                                <li><a class="dropdown-item" href="30_overfitting_underfitting.html">Overfitting & Underfitting</a></li>
                                <li><a class="dropdown-item" href="31_principle_component_analysis.html">Principle Component Analysis</a></li>
                                <li><a class="dropdown-item" href="32_p_value.html">P-Value</a></li>
                                <li><a class="dropdown-item" href="33_regularization.html">Regularization in ML</a></li>
                                <li><a class="dropdown-item" href="34_overfitting.html">Overfitting in ML</a></li>
                                <li><a class="dropdown-item" href="35_bias_variance.html">Bias & Variance</a></li>
                                <li><a class="dropdown-item" href="36_gradient_descent.html">Gradient Descent</a></li>
                                <li><a class="dropdown-item" href="37_cost_function.html">Cost Function</a></li>
                                <li><a class="dropdown-item" href="38_normalization.html">Normalization in ML</a></li>
                                <li><a class="dropdown-item" href="39_epoch_batch_iterations.html">Epoch, Batch & Iterations</a></li>
                                <li><a class="dropdown-item" href="40_feature_engineering.html">Feature Engineering</a></li>
                                <li><a class="dropdown-item" href="41_perceptron.html">Perceptron</a></li>
                                <li><a class="dropdown-item" href="42_data_science_vs_ml.html">Data Science vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="43_ml_vs_deep_learning.html">Machine Learning vs Deep Learning</a></li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../about.html">About Us</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../contact.html">Contact Us</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </nav>
    <div style="padding: 30px;font-size: 18px;">
    <h1 class="h1">Cross-Validation in Machine Learning</h1>
    <p>Cross-validation is a technique for validating the model efficiency by training it on the subset of input data
        and testing on previously unseen subset of the input data. <strong><em>We can also say that it is a technique to
                check how a statistical model generalizes to an independent dataset</em></strong>. </p>
    <p>In <a href="https://www.javatpoint.com/machine-learning">machine learning</a>, there is always the need to test
        the stability of the model. It means based only on the training dataset; we can't fit our model on the training
        dataset. For this purpose, we reserve a particular sample of the dataset, which was not part of the training
        dataset. After that, we test our model on that sample before deployment, and this complete process comes under
        cross-validation. This is something different from the general train-test split.</p>
    <p>Hence the basic steps of cross-validations are:</p>
    <ul class="points">
        <li>Reserve a subset of the dataset as a validation set.</li>
        <li>Provide the training to the model using the training dataset.</li>
        <li>Now, evaluate model performance using the validation set. If the model performs well with the validation
            set, perform the further step, else check for the issues.</li>
    </ul>
    <h2 class="h2">Methods used for Cross-Validation</h2>
    <p>There are some common methods that are used for cross-validation. These methods are given below:</p>
    <ol class="points">
        <li><strong>Validation Set Approach</strong></li>
        <li><strong>Leave-P-out cross-validation</strong></li>
        <li><strong>Leave one out cross-validation</strong></li>
        <li><strong>K-fold cross-validation</strong></li>
        <li><strong>Stratified k-fold cross-validation</strong></li>
    </ol>
    <h3 class="h3">Validation Set Approach</h3>
    <p>We divide our input dataset into a training set and test or validation set in the validation set approach. Both
        the subsets are given 50% of the dataset.</p>
    <p>But it has one of the big disadvantages that we are just using a 50% dataset to train our model, so the model may
        miss out to capture important information of the dataset. It also tends to give the underfitted model.</p>
    <h3 class="h3">Leave-P-out cross-validation</h3>
    <p>In this approach, the p datasets are left out of the training data. It means, if there are total n datapoints in
        the original input dataset, then n-p data points will be used as the training dataset and the p data points as
        the validation set. This complete process is repeated for all the samples, and the average error is calculated
        to know the effectiveness of the model.</p>
    <p>There is a disadvantage of this technique; that is, it can be computationally difficult for the large p.</p>
    <h3 class="h3">Leave one out cross-validation</h3>
    <p>This method is similar to the leave-p-out cross-validation, but instead of p, we need to take 1 dataset out of
        training. It means, in this approach, for each learning set, only one datapoint is reserved, and the remaining
        dataset is used to train the model. This process repeats for each datapoint. Hence for n samples, we get n
        different training set and n test set. It has the following features:</p>
    <ul class="points">
        <li>In this approach, the bias is minimum as all the data points are used.</li>
        <li>The process is executed for n times; hence execution time is high.</li>
        <li>This approach leads to high variation in testing the effectiveness of the model as we iteratively check
            against one data point.</li>
    </ul>
    <h3 class="h3">K-Fold Cross-Validation</h3>
    <p>K-fold cross-validation approach divides the input dataset into K groups of samples of equal sizes. These samples
        are called <strong>folds</strong>. For each learning set, the prediction function uses k-1 folds, and the rest
        of the folds are used for the test set. This approach is a very popular CV approach because it is easy to
        understand, and the output is less biased than other methods.</p>
    <p>The steps for k-fold cross-validation are:</p>
    <ul class="points">
        <li>Split the input dataset into K groups</li>
        <li>For each group:
            <ul class="points">
                <li>Take one group as the reserve or test data set.</li>
                <li>Use remaining groups as the training dataset</li>
                <li>Fit the model on the training set and evaluate the performance of the model using the test set.</li>
            </ul>
        </li>
    </ul>
    <p>Let's take an example of 5-folds cross-validation. So, the dataset is grouped into 5 folds. On 1<sup>st</sup>
        iteration, the first fold is reserved for test the model, and rest are used to train the model. On
        2<sup>nd</sup> iteration, the second fold is used to test the model, and rest are used to train the model. This
        process will continue until each fold is not used for the test fold.</p>
    <p>Consider the below diagram:</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/cross-validation.png"
        alt="Cross-Validation in Machine Learning" />
    <h3 class="h3">Stratified k-fold cross-validation</h3>
    <p>This technique is similar to k-fold cross-validation with some little changes. This approach works on
        stratification concept, it is a process of rearranging the data to ensure that each fold or group is a good
        representative of the complete dataset. To deal with the bias and variance, it is one of the best approaches.
    </p>
    <p>It can be understood with an example of housing prices, such that the price of some houses can be much high than
        other houses. To tackle such situations, a stratified k-fold cross-validation technique is useful.</p>
    <h3 class="h3">Holdout Method</h3>
    <p>This method is the simplest cross-validation technique among all. In this method, we need to remove a subset of
        the training data and use it to get prediction results by training it on the rest part of the dataset.</p>
    <p>The error that occurs in this process tells how well our model will perform with the unknown dataset. Although
        this approach is simple to perform, it still faces the issue of high variance, and it also produces misleading
        results sometimes.</p>
    <h2 class="h2">Comparison of Cross-validation to train/test split in Machine Learning</h2>
    <ul class="points">
        <li><strong>Train/test split:</strong> The input data is divided into two parts, that are training set and test
            set on a ratio of 70:30, 80:20, etc. It provides a high variance, which is one of the biggest disadvantages.
            <ul class="points">
                <li><strong>Training Data:</strong> The training data is used to train the model, and the dependent
                    variable is known.</li>
                <li><strong>Test Data:</strong> The test data is used to make the predictions from the model that is
                    already trained on the training data. This has the same features as training data but not the part
                    of that.</li>
            </ul>
        </li>
        <li><strong>Cross-Validation dataset:</strong> It is used to overcome the disadvantage of train/test split by
            splitting the dataset into groups of train/test splits, and averaging the result. It can be used if we want
            to optimize our model that has been trained on the training dataset for the best performance. It is more
            efficient as compared to train/test split as every observation is used for the training and testing both.
        </li>
    </ul>
    <h2 class="h2">Limitations of Cross-Validation</h2>
    <p>There are some limitations of the cross-validation technique, which are given below:</p>
    <ul class="points">
        <li>For the ideal conditions, it provides the optimum output. But for the inconsistent data, it may produce a
            drastic result. So, it is one of the big disadvantages of cross-validation, as there is no certainty of the
            type of data in machine learning.</li>
        <li>In predictive modeling, the data evolves over a period, due to which, it may face the differences between
            the training set and validation sets. Such as if we create a model for the prediction of stock market
            values, and the data is trained on the previous 5 years stock values, but the realistic future values for
            the next 5 years may drastically different, so it is difficult to expect the correct output for such
            situations.</li>
    </ul>
    <h2 class="h2">Applications of Cross-Validation</h2>
    <ul class="points">
        <li>This technique can be used to compare the performance of different predictive modeling methods.</li>
        <li>It has great scope in the medical research field.</li>
        <li>It can also be used for the meta-analysis, as it is already being used by the data scientists in the field
            of medical statistics.</li>
    </ul>
    <hr />
</div>
<div class="d-flex justify-content-between align-items-center my-4 mx-5">
        <a href="27_confusion_matrix.html"><button class="btn btn-danger" type="button">Previous</button></a>
        <a href="29_dimensionality_reduction.html"><button class="btn btn-success" type="button">Next</button></a>
    </div>
    <footer>
        <div class="container-fluid bg-dark text-bg-dark"
            style="height: 50px; display: flex; align-items: center; font-size: 15px; justify-content: center;">
            Copyright&nbsp; ©&nbsp; 2023&nbsp; StudyGenie&nbsp; -&nbsp; All Rights Reserved.
        </div>
    </footer>
</body>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
    crossorigin="anonymous"></script>
</html>