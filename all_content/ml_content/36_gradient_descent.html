<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gradient Descent</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet"
    integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
    <link rel="stylesheet" href="../../stylesheet.css">
    <link rel="stylesheet" type="text/css" href="https://static.javatpoint.com/link.css?v=6.0" async />
    <link rel="icon" type="image/x-icon" href="../icon/sg.png">
    <style>
        .active-submenu{
            font-weight: 500;
            background-color: darkgray;
        }
        .active-submenu:hover{
            font-weight: 500;
            background-color: darkgray;
        }
        a:hover{
            text-decoration: none;
        }
    </style>
</head>

<body style="background-image: radial-gradient(#b3d6e6, white, #b3d6e6);">
    <nav class="navbar navbar-dark bg-primary sticky-top" aria-label="Dark offcanvas navbar">
        <div class="container-fluid">
            <a class="navbar-brand" href="../../home.html">
                <img src="../icon/sg.png" alt="icon" style="height: 50px; width: 50px; margin-left: 20px;">
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="offcanvas"
                data-bs-target="#offcanvasNavbarDark" aria-controls="offcanvasNavbarDark"
                aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="offcanvas offcanvas-end text-bg-dark" tabindex="-1" id="offcanvasNavbarDark"
                aria-labelledby="offcanvasNavbarDarkLabel">
                <div class="offcanvas-header">
                    <h5 class="offcanvas-title" id="offcanvasNavbarDarkLabel">
                        <img src="../icon/sg.png" alt="icon" style="height: 50px;">
                    </h5>
                    <button type="button" class="btn-close btn-close-white" data-bs-dismiss="offcanvas"
                        aria-label="Close"></button>
                </div>
                <div class="offcanvas-body">
                    <ul class="navbar-nav justify-content-end flex-grow-1 pe-3">
                        <li class="nav-item">
                            <a class="nav-link" aria-current="page" href="../../home.html">Home</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Programming Language
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/c_page.html">C</a></li>
                                <li><a class="dropdown-item" href="../../first_page/c++_page.html">C++</a></li>
                                <li><a class="dropdown-item" href="../../first_page/java_page.html">Java</a></li>
                                <li><a class="dropdown-item" href="../../first_page/python_page.html">Python</a></li>
                            </ul>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Web Developement
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="../../first_page/html_page.html">HTML</a></li>
                                <li><a class="dropdown-item" href="../../first_page/css_page.html">CSS</a></li>
                                <li><a class="dropdown-item" href="../../first_page/javascript_page.html">JavaScript</a>
                                </li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../first_page/ml_page.html">Machine Learning</a>
                        </li>
                        <li class="nav-item dropdown">
                            <a class="nav-link active dropdown-toggle" href="#" role="button" data-bs-toggle="dropdown"
                                aria-expanded="false">
                                Machine Learning
                            </a>
                            <ul class="dropdown-menu">
                                <li><a class="dropdown-item" href="1_what_is_machine_learning.html">What is Machine Learning</a></li>
                                <li><a class="dropdown-item" href="2_application_of_ml.html">Applications of ML</a></li>
                                <li><a class="dropdown-item" href="3_life_cycle.html">Machine Learning Life Cycle</a></li>
                                <li><a class="dropdown-item" href="4_installing_anaconda_python.html">Installing Anaconda & Python</a></li>
                                <li><a class="dropdown-item" href="5_ai_vs_ml.html">Artificial Intelligence vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="6_datasets.html">Datasets</a></li>
                                <li><a class="dropdown-item" href="7_data_preprocessing.html">Data preprocessing</a></li>
                                <li><a class="dropdown-item" href="8_supervised_learning.html">Supervised Learning</a></li>
                                <li><a class="dropdown-item" href="9_unsupervised_learning.html">Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="10_supervised_vs_unsupervised.html">Supervised Learning vs Unsupervised Learning</a></li>
                                <li><a class="dropdown-item" href="11_regression_analysis.html">Regression Analysis</a></li>
                                <li><a class="dropdown-item" href="12_linear_regression.html">Linear Regression</a></li>
                                <li><a class="dropdown-item" href="13_simple_linear_regression.html">Simple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="14_multiple_linear_regression.html">Multiple Linear Regression</a></li>
                                <li><a class="dropdown-item" href="15_polynomial_regression.html">Polynomial Regression</a></li>
                                <li><a class="dropdown-item" href="16_classification_algorithm.html">Classification</a></li>
                                <li><a class="dropdown-item" href="17_logistic_regression.html">Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="18_knn.html">KNN Algorithm</a></li>
                                <li><a class="dropdown-item" href="19_svm.html">Support Vector Machine</a></li>
                                <li><a class="dropdown-item" href="20_naive_bayes.html">Naive Bayes</a></li>
                                <li><a class="dropdown-item" href="21_regression_vs_classification.html">Regression vs Classification</a></li>
                                <li><a class="dropdown-item" href="22_linear_reg_vs_logistic_reg.html">Linear Regression vs Logistic Regression</a></li>
                                <li><a class="dropdown-item" href="23_decision_tree.html">Decision Tree</a></li>
                                <li><a class="dropdown-item" href="24_random_forest.html">Random Forest</a></li>
                                <li><a class="dropdown-item" href="25_clustering.html">Clustering</a></li>
                                <li><a class="dropdown-item" href="26_k_means_clustering.html">K-Means Clustering</a></li>
                                <li><a class="dropdown-item" href="27_confusion_matrix.html">Confusion Matrix</a></li>
                                <li><a class="dropdown-item" href="28_cross_validation.html">Cross Validation</a></li>
                                <li><a class="dropdown-item" href="29_dimensionality_reduction.html">Dimensionality Reduction</a></li>
                                <li><a class="dropdown-item" href="30_overfitting_underfitting.html">Overfitting & Underfitting</a></li>
                                <li><a class="dropdown-item" href="31_principle_component_analysis.html">Principle Component Analysis</a></li>
                                <li><a class="dropdown-item" href="32_p_value.html">P-Value</a></li>
                                <li><a class="dropdown-item" href="33_regularization.html">Regularization in ML</a></li>
                                <li><a class="dropdown-item" href="34_overfitting.html">Overfitting in ML</a></li>
                                <li><a class="dropdown-item" href="35_bias_variance.html">Bias & Variance</a></li>
                                <li><a class="dropdown-item active-submenu" href="36_gradient_descent.html">Gradient Descent</a></li>
                                <li><a class="dropdown-item" href="37_cost_function.html">Cost Function</a></li>
                                <li><a class="dropdown-item" href="38_normalization.html">Normalization in ML</a></li>
                                <li><a class="dropdown-item" href="39_epoch_batch_iterations.html">Epoch, Batch & Iterations</a></li>
                                <li><a class="dropdown-item" href="40_feature_engineering.html">Feature Engineering</a></li>
                                <li><a class="dropdown-item" href="41_perceptron.html">Perceptron</a></li>
                                <li><a class="dropdown-item" href="42_data_science_vs_ml.html">Data Science vs Machine Learning</a></li>
                                <li><a class="dropdown-item" href="43_ml_vs_deep_learning.html">Machine Learning vs Deep Learning</a></li>
                            </ul>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../about.html">About Us</a>
                        </li>
                        <li class="nav-item">
                            <a class="nav-link" href="../../contact.html">Contact Us</a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>
    </nav>
    <div style="padding: 30px;font-size: 18px;">
    <h1 class="h1">Gradient Descent in Machine Learning</h1>
    <p>Gradient Descent is known as one of the most commonly used optimization algorithms to train machine learning
        models by means of minimizing errors between actual and expected results. Further, gradient descent is also used
        to train Neural Networks.</p>
    <p>In mathematical terminology, Optimization algorithm refers to the task of minimizing/maximizing an objective
        function f(x) parameterized by x. Similarly, in machine learning, optimization is the task of minimizing the
        cost function parameterized by the model's parameters. The main objective of gradient descent is to minimize the
        convex function using iteration of parameter updates. Once these machine learning models are optimized, these
        models can be used as powerful tools for Artificial Intelligence and various computer science applications.</p>
    <p>In this tutorial on Gradient Descent in Machine Learning, we will learn in detail about gradient descent, the
        role of cost functions specifically as a barometer within Machine Learning, types of gradient descents, learning
        rates, etc.</p>
    <h2 class="h2">What is Gradient Descent or Steepest Descent?</h2>
    <p>Gradient descent was initially discovered by <strong>"Augustin-Louis Cauchy"</strong> in mid of 18th century.
        <strong><em>Gradient Descent is defined as one of the most commonly used iterative optimization algorithms of
                machine learning to train the machine learning and deep learning models. It helps in finding the local
                minimum of a function.</em></strong></p>
    <p>The best way to define the local minimum or local maximum of a function using gradient descent is as follows:</p>
    <ul class="points">
        <li>If we move towards a negative gradient or away from the gradient of the function at the current point, it
            will give the <strong>local minimum</strong> of that function.</li>
        <li>Whenever we move towards a positive gradient or towards the gradient of the function at the current point,
            we will get the <strong>local maximum</strong> of that function.</li>
    </ul>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/gradient-descent-in-machine-learning1.png"
        alt="Gradient Descent in Machine Learning" />
    <p>This entire procedure is known as Gradient Ascent, which is also known as steepest descent. <strong><em>The main
                objective of using a gradient descent algorithm is to minimize the cost function using
                iteration.</em></strong> To achieve this goal, it performs two steps iteratively:</p>
    <ul class="points">
        <li>Calculates the first-order derivative of the function to compute the gradient or slope of that function.
        </li>
        <li>Move away from the direction of the gradient, which means slope increased from the current point by alpha
            times, where Alpha is defined as Learning Rate. It is a tuning parameter in the optimization process which
            helps to decide the length of the steps.</li>
    </ul>
    <h3 class="h3">What is Cost-function?</h3>
    <p><strong><em>The cost function is defined as the measurement of difference or error between actual values and
                expected values at the current position and present in the form of a single real number.</em></strong>
        It helps to increase and improve machine learning efficiency by providing feedback to this model so that it can
        minimize error and find the local or global minimum. Further, it continuously iterates along the direction of
        the negative gradient until the cost function approaches zero. At this steepest descent point, the model will
        stop learning further. Although cost function and loss function are considered synonymous, also there is a minor
        difference between them. The slight difference between the loss function and the cost function is about the
        error within the training of machine learning models, as loss function refers to the error of one training
        example, while a cost function calculates the average error across an entire training set.</p>
    <p>The cost function is calculated after making a hypothesis with initial parameters and modifying these parameters
        using gradient descent algorithms over known data to reduce the cost function.</p>
    <p>Hypothesis:</p>
    <p>Parameters:</p>
    <p>Cost function:</p>
    <p>Goal:</p>
    <h3 class="h3">How does Gradient Descent work?</h3>
    <p>Before starting the working principle of gradient descent, we should know some basic concepts to find out the
        slope of a line from linear regression. The equation for simple linear regression is given as:</p>
    <div class="codeblock"><textarea name="code" class="xml">
       Y=mX+c
</textarea></div>
    <p>Where 'm' represents the slope of the line, and 'c' represents the intercepts on the y-axis.</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/gradient-descent-in-machine-learning2.png"
        alt="Gradient Descent in Machine Learning" />
    <p>The starting point(shown in above fig.) is used to evaluate the performance as it is considered just as an
        arbitrary point. At this starting point, we will derive the first derivative or slope and then use a tangent
        line to calculate the steepness of this slope. Further, this slope will inform the updates to the parameters
        (weights and bias).</p>
    <p>The slope becomes steeper at the starting point or arbitrary point, but whenever new parameters are generated,
        then steepness gradually reduces, and at the lowest point, it approaches the lowest point, which is called
        <strong>a point of convergence.</strong></p>
    <p>The main objective of gradient descent is to minimize the cost function or the error between expected and actual.
        To minimize the cost function, two data points are required:</p>
    <ul class="points">
        <li><strong>Direction & Learning Rate</strong></li>
    </ul>
    <p>These two factors are used to determine the partial derivative calculation of future iteration and allow it to
        the point of convergence or local minimum or global minimum. Let's discuss learning rate factors in brief;</p>
    <h3 class="h3">Learning Rate:</h3>
    <p>It is defined as the step size taken to reach the minimum or lowest point. This is typically a small value that
        is evaluated and updated based on the behavior of the cost function. If the learning rate is high, it results in
        larger steps but also leads to risks of overshooting the minimum. At the same time, a low learning rate shows
        the small step sizes, which compromises overall efficiency but gives the advantage of more precision.</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/gradient-descent-in-machine-learning3.png"
        alt="Gradient Descent in Machine Learning" />
    <h2 class="h2">Types of Gradient Descent</h2>
    <p>Based on the error in various training models, the Gradient Descent learning algorithm can be divided into
        <strong>Batch gradient descent, stochastic gradient descent, and mini-batch gradient descent.</strong> Let's
        understand these different types of gradient descent:</p>
    <h3 class="h3">1. Batch Gradient Descent:</h3>
    <p>Batch gradient descent (BGD) is used to find the error for each point in the training set and update the model
        after evaluating all training examples. This procedure is known as the training epoch. In simple words, it is a
        greedy approach where we have to sum over all examples for each update.</p>
    <p><strong>Advantages of Batch gradient descent:</strong></p>
    <ul class="points">
        <li>It produces less noise in comparison to other gradient descent.</li>
        <li>It produces stable gradient descent convergence.</li>
        <li>It is Computationally efficient as all resources are used for all training samples.</li>
    </ul>
    <h3 class="h3">2. Stochastic gradient descent</h3>
    <p>Stochastic gradient descent (SGD) is a type of gradient descent that runs one training example per iteration. Or
        in other words, it processes a training epoch for each example within a dataset and updates each training
        example's parameters one at a time. As it requires only one training example at a time, hence it is easier to
        store in allocated memory. However, it shows some computational efficiency losses in comparison to batch
        gradient systems as it shows frequent updates that require more detail and speed. Further, due to frequent
        updates, it is also treated as a noisy gradient. However, sometimes it can be helpful in finding the global
        minimum and also escaping the local minimum.</p>
    <p><strong>Advantages of Stochastic gradient descent:</strong></p>
    <p>In Stochastic gradient descent (SGD), learning happens on every example, and it consists of a few advantages over
        other gradient descent.</p>
    <ul class="points">
        <li>It is easier to allocate in desired memory.</li>
        <li>It is relatively fast to compute than batch gradient descent.</li>
        <li>It is more efficient for large datasets.</li>
    </ul>
    <h3 class="h3">3. MiniBatch Gradient Descent:</h3>
    <p>Mini Batch gradient descent is the combination of both batch gradient descent and stochastic gradient descent. It
        divides the training datasets into small batch sizes then performs the updates on those batches separately.
        Splitting training datasets into smaller batches make a balance to maintain the computational efficiency of
        batch gradient descent and speed of stochastic gradient descent. Hence, we can achieve a special type of
        gradient descent with higher computational efficiency and less noisy gradient descent.</p>
    <p><strong>Advantages of Mini Batch gradient descent:</strong></p>
    <ul class="points">
        <li>It is easier to fit in allocated memory.</li>
        <li>It is computationally efficient.</li>
        <li>It produces stable gradient descent convergence.</li>
    </ul>
    <h2 class="h2">Challenges with the Gradient Descent</h2>
    <p>Although we know Gradient Descent is one of the most popular methods for optimization problems, it still also has
        some challenges. There are a few challenges as follows:</p>
    <h3 class="h3">1. Local Minima and Saddle Point:</h3>
    <p>For convex problems, gradient descent can find the global minimum easily, while for non-convex problems, it is
        sometimes difficult to find the global minimum, where the machine learning models achieve the best results.</p>
    <img src="https://static.javatpoint.com/tutorial/machine-learning/images/gradient-descent-in-machine-learning4.png"
        alt="Gradient Descent in Machine Learning" />
    <p>Whenever the slope of the cost function is at zero or just close to zero, this model stops learning further.
        Apart from the global minimum, there occur some scenarios that can show this slop, which is saddle point and
        local minimum. Local minima generate the shape similar to the global minimum, where the slope of the cost
        function increases on both sides of the current points.</p>
    <p>In contrast, with saddle points, the negative gradient only occurs on one side of the point, which reaches a
        local maximum on one side and a local minimum on the other side. The name of a saddle point is taken by that of
        a horse's saddle.</p>
    <p>The name of local minima is because the value of the loss function is minimum at that point in a local region. In
        contrast, the name of the global minima is given so because the value of the loss function is minimum there,
        globally across the entire domain the loss function.</p>
    <h3 class="h3">2. Vanishing and Exploding Gradient</h3>
    <p>In a deep neural network, if the model is trained with gradient descent and backpropagation, there can occur two
        more issues other than local minima and saddle point.</p>
    <h3 class="h4">Vanishing Gradients:</h3>
    <p>Vanishing Gradient occurs when the gradient is smaller than expected. During backpropagation, this gradient
        becomes smaller that causing the decrease in the learning rate of earlier layers than the later layer of the
        network. Once this happens, the weight parameters update until they become insignificant.</p>
    <h3 class="h4">Exploding Gradient:</h3>
    <p>Exploding gradient is just opposite to the vanishing gradient as it occurs when the Gradient is too large and
        creates a stable model. Further, in this scenario, model weight increases, and they will be represented as NaN.
        This problem can be solved using the dimensionality reduction technique, which helps to minimize complexity
        within the model.</p>
    <hr />
</div>
<div class="d-flex justify-content-between align-items-center my-4 mx-5">
        <a href="35_bias_variance.html"><button class="btn btn-danger" type="button">Previous</button></a>
        <a href="37_cost_function.html"><button class="btn btn-success" type="button">Next</button></a>
    </div>
    <footer>
        <div class="container-fluid bg-dark text-bg-dark"
            style="height: 50px; display: flex; align-items: center; font-size: 15px; justify-content: center;">
            Copyright&nbsp; ©&nbsp; 2023&nbsp; StudyGenie&nbsp; -&nbsp; All Rights Reserved.
        </div>
    </footer>
</body>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"
    integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL"
    crossorigin="anonymous"></script>
</html>